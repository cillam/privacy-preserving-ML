{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis Model with Differential Privacy Integration\n",
    "### CYBER/DATASCI 233\n",
    "### Students: Wendy Matta, Armaan Hiranandani, Priscilla Miller"
   ],
   "id": "db9edfa8c81baa53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    "If using Colab, uncomment below cell. Tensorflow Privacy 0.9.0 requires Python 3.9 or 3.10."
   ],
   "id": "fe380c697eb3a50f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "# Install dependencies\n",
    "# !pip install -q transformers==4.17\n",
    "# !pip install tensorflow-macos \n",
    "# !pip install tensorflow-metal\n",
    "# !pip install tensorflow==2.15.0 --quiet\n",
    "# !pip install tensorflow-text==2.15.0 --quiet\n",
    "# !pip install tensorflow-privacy\n",
    "# !pip install numpy==1.26.4\n",
    "# !pip install pandas==2.1.1\n"
   ],
   "id": "5-E2sfMdYE8L"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Below cell suppresses warnings. Do not run if you would prefer to see warnings. ",
   "id": "f1195892060890e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T22:07:53.042342Z",
     "start_time": "2025-04-08T22:07:52.878067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Suppress warnings from all Python libraries\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure basic logging level\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Target specific libraries\n",
    "for module in ['tensorflow', 'absl', 'transformers', 'tensorboard']:\n",
    "    logging.getLogger(module).setLevel(logging.ERROR)\n",
    "\n",
    "from transformers import logging as transformers_logging\n",
    "transformers_logging.set_verbosity_error()\n",
    "\n",
    "# Suppress the specific BERT layer warnings\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'"
   ],
   "id": "ca385959c79f25ce",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import TensorFlow and check if it detects GPU",
   "id": "698f93442718e744"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T22:07:55.183667Z",
     "start_time": "2025-04-08T22:07:53.043453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ],
   "id": "51c8ac0b9958ef55",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import dependencies",
   "id": "95353e0314898f50"
  },
  {
   "metadata": {
    "id": "initial_id",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "outputId": "4b19d334-db27-4e16-a8ff-8b5c29479566",
    "ExecuteTime": {
     "end_time": "2025-04-08T22:07:55.642755Z",
     "start_time": "2025-04-08T22:07:55.184395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import libraries/modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_privacy\n",
    "from tensorflow_privacy.privacy.optimizers import dp_optimizer_keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import TFBertModel, BertTokenizer, TFDistilBertModel, DistilBertTokenizer\n",
    "from tensorflow_privacy.privacy.analysis import compute_noise_from_budget_lib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from mia_callback import MembershipInferenceCallback"
   ],
   "id": "initial_id",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T22:07:55.645675Z",
     "start_time": "2025-04-08T22:07:55.643941Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\" Tensorflow_privacy version: {tensorflow_privacy.__version__}\")",
   "id": "fa4538f504ac4844",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Loading and Preprocessing",
   "id": "6e08f7e666efd421"
  },
  {
   "metadata": {
    "id": "6c034bf52ca668ee",
    "ExecuteTime": {
     "end_time": "2025-04-08T22:07:55.647787Z",
     "start_time": "2025-04-08T22:07:55.646303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set global variables\n",
    "MAX_LENGTH = 512\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 3"
   ],
   "id": "6c034bf52ca668ee",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load filtered and balanced data sample from pickle file",
   "id": "265128f2ce0a527e"
  },
  {
   "metadata": {
    "id": "8482f4150c580251",
    "ExecuteTime": {
     "end_time": "2025-04-08T22:07:55.654150Z",
     "start_time": "2025-04-08T22:07:55.648408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load dataset\n",
    "reviews_df = pd.read_pickle('data/resto_reviews_3classes.pkl')"
   ],
   "id": "8482f4150c580251",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "id": "ceae08293524ca92",
    "ExecuteTime": {
     "end_time": "2025-04-08T22:07:55.664002Z",
     "start_time": "2025-04-08T22:07:55.654781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# shuffle dataframe\n",
    "reviews_df = reviews_df.sample(frac=1, random_state=333)\n",
    "# Reset index\n",
    "reviews_df = reviews_df.reset_index(drop=True)\n",
    "\n",
    "# Preview dataframe\n",
    "display(reviews_df)"
   ],
   "id": "ceae08293524ca92",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Label encode categorical values as integers and prep text for tokenization",
   "id": "f0bcb0ac41f7e437"
  },
  {
   "metadata": {
    "id": "be0645df82cc8",
    "ExecuteTime": {
     "end_time": "2025-04-08T22:07:55.674184Z",
     "start_time": "2025-04-08T22:07:55.664625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Label encode forms\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the column\n",
    "reviews_df['sentiment_encoded'] = le.fit_transform(reviews_df['sentiment'])\n",
    "\n",
    "# Calculate the desired training set size (nearest multiple of 32 to 60% of data)\n",
    "# This is so batch sizes can be divided evenly for DP microbatches\n",
    "total_samples = len(reviews_df)\n",
    "train_size = int(total_samples * 0.6)\n",
    "train_size_adjusted = (train_size // 32) * 32  # Round down to nearest multiple of 32\n",
    "\n",
    "# Use the adjusted size in train_test_split\n",
    "train_ratio = train_size_adjusted / total_samples\n",
    "remaining_ratio = 1 - train_ratio\n",
    "\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(reviews_df['text'], reviews_df['sentiment_encoded'], test_size=remaining_ratio, random_state=123, stratify=reviews_df['sentiment_encoded'])\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.50, random_state=1234,\n",
    "    stratify=y_temp)\n",
    "\n",
    "# lowercase text if using uncased pretrained model\n",
    "x_train = x_train.str.lower()\n",
    "x_val = x_val.str.lower()\n",
    "x_test = x_test.str.lower()\n",
    "\n",
    "# Calculate delta\n",
    "# this calculates the delta parameter to calculate noise multiplier based on epsilon\n",
    "n_examples = len(x_train)\n",
    "DELTA = round(1/n_examples, 5)\n"
   ],
   "id": "be0645df82cc8",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Functions for tokenization and for confusion matrix (used to visualize model results later on)",
   "id": "47a96ca0cc7e10cc"
  },
  {
   "metadata": {
    "id": "76a6ea9c8d50ce8e",
    "ExecuteTime": {
     "end_time": "2025-04-08T22:07:55.678860Z",
     "start_time": "2025-04-08T22:07:55.674952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create bert encoder function\n",
    "def bert_encode(texts, tokenizer, max_len=MAX_LENGTH):\n",
    "    # Convert texts to strings\n",
    "    texts_str = [str(text) for text in texts]\n",
    "\n",
    "    # Tokenize texts\n",
    "    tokenized = tokenizer(texts_str,\n",
    "                         max_length=max_len,\n",
    "                         truncation=True,\n",
    "                         padding='max_length',\n",
    "                         return_tensors='tf')\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# select tokenizer\n",
    "def select_tokenizer(pretrained):\n",
    "    # Extract model type from checkpoint name\n",
    "    if 'distilbert' in pretrained.lower():\n",
    "        return DistilBertTokenizer.from_pretrained(pretrained)\n",
    "    elif 'bert' in pretrained.lower():\n",
    "        return BertTokenizer.from_pretrained(pretrained)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {pretrained}\")\n",
    "\n",
    "# Confusion matrix display function\n",
    "def plot_confusion_matrix(cm, class_names=['Negative', 'Neutral', 'Positive']):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Create heatmap with numerical values inside cells\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    # Add accuracy in each cell as percentage\n",
    "    total = np.sum(cm)\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm[i])):\n",
    "            plt.text(j + 0.5, i + 0.7, f'({cm[i, j]/total*100:.1f}%)',\n",
    "                    ha='center', color='black' if cm[i, j] < cm.max()/2 else 'white')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return plt"
   ],
   "id": "76a6ea9c8d50ce8e",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Select models and call tokenization functions",
   "id": "7cf41c68cebfdeb0"
  },
  {
   "metadata": {
    "id": "d2b0e213f32a1e04",
    "ExecuteTime": {
     "end_time": "2025-04-08T22:08:02.276053Z",
     "start_time": "2025-04-08T22:07:55.680664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select pretrained\n",
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "# Load tokenizer\n",
    "bert_tokenizer = select_tokenizer(model_checkpoint)\n",
    "print(f\"Using {type(bert_tokenizer).__name__} with {model_checkpoint}\")\n",
    "\n",
    "# Tokenize dataset\n",
    "train_encodings = bert_encode(x_train, bert_tokenizer)\n",
    "val_encodings = bert_encode(x_val, bert_tokenizer)\n",
    "test_encodings = bert_encode(x_test, bert_tokenizer)\n",
    "train_labels = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
    "val_labels = tf.convert_to_tensor(y_val, dtype=tf.int32)\n",
    "test_labels = tf.convert_to_tensor(y_test, dtype=tf.int32)"
   ],
   "id": "d2b0e213f32a1e04",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check shapes",
   "id": "c59b5a1df239a4c0"
  },
  {
   "metadata": {
    "id": "47e612f8678456fa",
    "ExecuteTime": {
     "end_time": "2025-04-08T22:08:02.279354Z",
     "start_time": "2025-04-08T22:08:02.276871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Type of train_encodings:\", type(train_encodings))\n",
    "print(\"Keys of train_encodings:\", train_encodings.keys())\n",
    "print(\"train_y shape:\", train_labels.shape)\n",
    "print(\"train_y dtype:\", train_labels.dtype)\n",
    "print(\"train_y unique values:\", np.unique(train_labels))\n",
    "print(\"Number of classes:\", len(np.unique(train_labels)))"
   ],
   "id": "47e612f8678456fa",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "id": "7f738bcdec6657cd"
   },
   "cell_type": "markdown",
   "source": [
    "## Model Architecture with Differential Privacy Integration\n",
    "\n",
    "build_dp_model function defines model architecture for both the non-DP and DP models."
   ],
   "id": "7f738bcdec6657cd"
  },
  {
   "metadata": {
    "id": "348bc13154fe2511",
    "ExecuteTime": {
     "end_time": "2025-04-08T22:08:04.299494Z",
     "start_time": "2025-04-08T22:08:02.279994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_dp_model(bert_base,\n",
    "                trainable=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                num_classes=NUM_CLASSES,\n",
    "                hidden_sizes=[128],\n",
    "                #dropout=[0.3], # Removed to increase overfitting\n",
    "                learning_rate=0.0001,\n",
    "                epsilon=None,\n",
    "                delta=DELTA,\n",
    "                l2_norm_clip=1.0,\n",
    "                num_microbatches=1):\n",
    "\n",
    "    # Select appropriate model type based on the checkpoint name\n",
    "    if 'distilbert' in bert_base:\n",
    "        bert_model = TFDistilBertModel.from_pretrained(bert_base)\n",
    "    else:\n",
    "        bert_model = TFBertModel.from_pretrained(bert_base)\n",
    "\n",
    "    # Unfreeze all layers to make model overfit\n",
    "    # Overfitting provides more evidence that DP is working (less memorization)\n",
    "    bert_model.trainable = trainable  # This sets everything trainable\n",
    "    \n",
    "    # Unfreeze only final layers to save on compute\n",
    "    if 'distilbert' in bert_base:\n",
    "        for i in range(0, len(bert_model.distilbert.transformer.layer) - 4):\n",
    "            bert_model.distilbert.transformer.layer[i].trainable = False  \n",
    "    else:\n",
    "        for i in range(0, len(bert_model.encoder.layer) - 4):\n",
    "            bert_model.encoder.layer[i].trainable = False  \n",
    "\n",
    "    # Create input layers\n",
    "    input_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='input_ids_layer')\n",
    "    attention_mask = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask_layer')\n",
    "\n",
    "    if 'distilbert' in bert_base:\n",
    "        bert_inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "        model_inputs = [input_ids, attention_mask]\n",
    "    else:\n",
    "        token_type_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "        bert_inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "        model_inputs = [input_ids, token_type_ids, attention_mask]\n",
    "\n",
    "    bert_output = bert_model(bert_inputs)\n",
    "\n",
    "    # Get pooler output\n",
    "    if 'distilbert' in bert_base:\n",
    "        sequence_output = bert_output[0]\n",
    "        pooler_output = sequence_output[:, 0, :]\n",
    "    else:\n",
    "        pooler_output = bert_output[1]\n",
    "\n",
    "    # Add multiple hidden layers with decreasing sizes\n",
    "    # Current architecture uses only one hidden layer\n",
    "    hidden = pooler_output\n",
    "    for i, hidden_size in enumerate(hidden_sizes):\n",
    "        hidden = tf.keras.layers.Dense(hidden_size, activation='relu')(hidden)\n",
    "        # if i < 1:\n",
    "        #     hidden = tf.keras.layers.Dropout(dropout[i], name=f'dropout_{i}')(hidden)\n",
    "\n",
    "    class_output = tf.keras.layers.Dense(num_classes, activation=None, name=\"classification_layer\")(hidden)\n",
    "\n",
    "    model = tf.keras.Model(inputs=model_inputs, outputs=class_output)\n",
    "    \n",
    "    \n",
    "    if epsilon is not None:\n",
    "        # Calculate noise multiplier\n",
    "        noise = compute_noise_from_budget_lib.compute_noise(n_examples, BATCH_SIZE, epsilon, EPOCHS, delta, 0.1)\n",
    "\n",
    "        optimizer = dp_optimizer_keras.DPKerasAdamOptimizer(\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise,\n",
    "            num_microbatches=num_microbatches,\n",
    "            learning_rate=learning_rate)\n",
    "\n",
    "        # For DP: Use non-reduced sparse categorical loss (calculates loss for each data point)\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Standard optimizer (no privacy)\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        # For non-DP: Use standard reduced loss\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        print(\"No privacy protection applied\")\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Obtain instance of pretrained model\n",
    "dp_model = build_dp_model(model_checkpoint)\n",
    "dp_model.summary()"
   ],
   "id": "348bc13154fe2511",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T22:08:05.389467Z",
     "start_time": "2025-04-08T22:08:04.300138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deletes instance of previous compiled model\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "# Short delay to ensure cleanup completes\n",
    "time.sleep(1)"
   ],
   "id": "93144c52f80443bf",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "id": "37e7dc1b5df2a1d5"
   },
   "cell_type": "markdown",
   "source": [
    "## Experiments\n",
    "\n",
    "run_dp_hyperparameter_study function loops through different clip norms to find the best one for our model architecture. Per tensorflow documentation, the clip norm can have varying effects on the DP models, either increasing or decreasing utility. We selected the best C based on validation accuracy after 3 epochs using a fixed epsilon of 1. \n"
   ],
   "id": "37e7dc1b5df2a1d5"
  },
  {
   "metadata": {
    "id": "dcab80363126c1c9",
    "ExecuteTime": {
     "end_time": "2025-04-08T22:08:05.418157Z",
     "start_time": "2025-04-08T22:08:05.392643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_dp_hyperparameter_study(train_x, train_y, val_x, val_y, test_x, test_y, selected_model, ablation_epochs=1):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Prepare inputs based on model type\n",
    "    if 'distilbert' in model_checkpoint:\n",
    "        train_inputs = [train_x.input_ids, train_x.attention_mask]\n",
    "        val_inputs = [val_x.input_ids, val_x.attention_mask]\n",
    "        test_inputs = [test_x.input_ids, test_x.attention_mask]\n",
    "    else:\n",
    "        train_inputs = [train_x.input_ids, train_x.token_type_ids, train_x.attention_mask]\n",
    "        val_inputs = [val_x.input_ids, val_x.token_type_ids, val_x.attention_mask]\n",
    "        test_inputs = [test_x.input_ids, test_x.token_type_ids, test_x.attention_mask]\n",
    "\n",
    "\n",
    "    # Clipping norm study\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Pick clipping norm\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    clip_norms = [0.3, 0.5, 1.0, 1.5]\n",
    "    for clip_norm in clip_norms:\n",
    "        print(f\"\\nTesting clipping norm: {clip_norm}\")\n",
    "\n",
    "        # Use a fixed epsilon and microbatches for this test\n",
    "        epsilon = 1.0\n",
    "        microbatches = 1\n",
    "\n",
    "       # Build model with specified privacy budget\n",
    "        model = build_dp_model(\n",
    "            selected_model,\n",
    "            epsilon=epsilon,\n",
    "            l2_norm_clip=clip_norm,\n",
    "            num_microbatches=microbatches\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_inputs,\n",
    "            train_y,\n",
    "            validation_data=(val_inputs, val_y),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=ablation_epochs,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_loss, test_acc = model.evaluate(test_inputs, test_y, verbose=0)\n",
    "\n",
    "        # Get predictions and calculate F1 score\n",
    "        y_pred = np.argmax(model.predict(test_inputs), axis=1)\n",
    "        f1 = f1_score(test_y, y_pred, average='weighted')\n",
    "\n",
    "        print(f\"Results for clip_norm={clip_norm}:\")\n",
    "        print(f\"Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            'parameter': 'clip_norm',\n",
    "            'value': clip_norm,\n",
    "            'epsilon': epsilon,\n",
    "            'microbatches': microbatches,\n",
    "            'accuracy': test_acc,\n",
    "            'f1_score': f1,\n",
    "            'val_accuracy': max(history.history['val_accuracy'])})\n",
    "\n",
    "        # Deletes instance of previous compiled model\n",
    "        # Since we are looping through multiple sequential model builds\n",
    "        # this keeps memory clear so the next model can be built\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        # Short delay to ensure cleanup completes\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Find best clip_norm\n",
    "    best_clip_results = [r for r in results if r['parameter'] == 'clip_norm']\n",
    "    best_clip_norm = max(best_clip_results, key=lambda x: x['f1_score'])['value']\n",
    "    print(f\"\\nBest configuration:\")\n",
    "    print(f\"- Clipping norm: {best_clip_norm}\")\n",
    "\n",
    "    # Create DataFrame from results\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Plot results\n",
    "    plot_ablation_results(results_df)\n",
    "\n",
    "    return results_df, best_clip_norm\n",
    "\n",
    "# function to plot the results of the clipping norm study\n",
    "def plot_ablation_results(results_df):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "\n",
    "    # Plot clipping norm results\n",
    "    clip_norm_results = results_df[results_df['parameter'] == 'clip_norm']\n",
    "    plt.plot(clip_norm_results['value'], clip_norm_results['accuracy'], 'o-', label='Accuracy')\n",
    "    plt.plot(clip_norm_results['value'], clip_norm_results['f1_score'], 'd-', label='F1 Score')\n",
    "    plt.title('Effect of Clipping Norm (C) on Model Performance')\n",
    "    plt.xlabel('Clipping Norm')\n",
    "    plt.ylabel('Performance Metric')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dp_hyperparameter_ablation.png', dpi=300)\n",
    "    plt.show()"
   ],
   "id": "dcab80363126c1c9",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run hyperparameter search function to find best C (clipping norm) value",
   "id": "8aefb5e26c6e6831"
  },
  {
   "metadata": {
    "id": "198b1da68379f41b",
    "ExecuteTime": {
     "end_time": "2025-04-08T23:10:57.171126Z",
     "start_time": "2025-04-08T22:08:05.420461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the simplified hyperparameter ablation study\n",
    "clipnorm_results_df, best_clip_norm = run_dp_hyperparameter_study(\n",
    "    train_encodings, train_labels,\n",
    "    val_encodings, val_labels,\n",
    "    test_encodings, test_labels,\n",
    "    model_checkpoint,\n",
    "    ablation_epochs=EPOCHS//2\n",
    ")\n",
    "\n",
    "# Display summary table\n",
    "display(clipnorm_results_df)\n",
    "\n",
    "print(f\"- Best clipping norm: {best_clip_norm}\")"
   ],
   "id": "198b1da68379f41b",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "run_dp_experiments function loops through epsilon values, builds model based on the provided epsilon value and with MIA callback, trains each model for 10 epochs, and returns results of training and MIA for all models. This section also provides functions to visualize results.",
   "id": "c7218deaafc9d8f8"
  },
  {
   "metadata": {
    "id": "fe9bef310fc53e85",
    "ExecuteTime": {
     "end_time": "2025-04-08T23:10:57.184826Z",
     "start_time": "2025-04-08T23:10:57.172079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run experiments with different privacy budgets\n",
    "def run_dp_experiments(train_x, train_y, val_x, val_y, test_x, test_y, epsilons=[None], set_microbatch=1, clip_norm=0.5):\n",
    "    results = []\n",
    "    # Set up inputs based on model type\n",
    "    if 'distilbert' in model_checkpoint:\n",
    "        train_inputs = [train_x.input_ids, train_x.attention_mask]\n",
    "        val_inputs = [val_x.input_ids, val_x.attention_mask]\n",
    "        test_inputs = [test_x.input_ids, test_x.attention_mask]\n",
    "    else:\n",
    "        train_inputs = [train_x.input_ids, train_x.token_type_ids, train_x.attention_mask]\n",
    "        val_inputs = [val_x.input_ids, val_x.token_type_ids, val_x.attention_mask]\n",
    "        test_inputs = [test_x.input_ids, test_x.token_type_ids, test_x.attention_mask]\n",
    "\n",
    "\n",
    "    for epsilon in epsilons:\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"Training with epsilon = {epsilon if epsilon is not None else 'No DP'}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "\n",
    "        # Build model with specified privacy budget\n",
    "        model = build_dp_model(\n",
    "            model_checkpoint,\n",
    "            epsilon=epsilon,\n",
    "            l2_norm_clip=clip_norm,\n",
    "            num_microbatches=set_microbatch\n",
    "        )\n",
    "        \n",
    "        # Create instance of model inference attack callback\n",
    "        mia_callback = MembershipInferenceCallback(\n",
    "            train_inputs=train_inputs,\n",
    "            train_labels=train_y,\n",
    "            val_inputs=val_inputs,\n",
    "            val_labels=val_y,\n",
    "            run_epochs=[1, 5, 10])\n",
    "        \n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_inputs,\n",
    "            train_y,\n",
    "            validation_data=(val_inputs, val_y),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[mia_callback],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_loss, test_acc = model.evaluate(test_inputs, test_y, verbose=0)\n",
    "\n",
    "        # Get predictions and calculate F1 score\n",
    "        y_pred = np.argmax(model.predict(test_inputs), axis=1)\n",
    "        f1 = f1_score(test_y, y_pred, average='weighted')\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        cm = confusion_matrix(test_y, y_pred)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'epsilon': epsilon,\n",
    "            'accuracy': test_acc,\n",
    "            'f1_score': f1,\n",
    "            'val_accuracy': max(history.history['val_accuracy']),\n",
    "            'training_history': history.history,\n",
    "            'mia_results': mia_callback.attack_results,\n",
    "            'mia_predictions': mia_callback.epoch_predictions,\n",
    "            'epoch_f1_scores': mia_callback.f1_per_epoch, \n",
    "        })\n",
    "\n",
    "        print(f\"Test results for epsilon={epsilon if epsilon is not None else 'No DP'}:\")\n",
    "        print(f\"Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "        # Show confusion matrix\n",
    "        plot_confusion_matrix(cm)\n",
    "        plt.show()\n",
    "        \n",
    "        # cleanup\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        # Short delay to ensure cleanup completes\n",
    "        time.sleep(1)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Plot results to visualize privacy-utility tradeoff\n",
    "def plot_privacy_utility_tradeoff(results):\n",
    "    # Convert None to \"No DP\" for plotting\n",
    "    epsilon_labels = [\"No DP\" if r['epsilon'] is None else str(r['epsilon']) for r in results]\n",
    "    accuracies = [r['accuracy'] for r in results]\n",
    "    f1_scores = [r['f1_score'] for r in results]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot privacy-utility tradeoff\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epsilon_labels, accuracies, 'o-', label='Accuracy')\n",
    "    plt.plot(epsilon_labels, f1_scores, 'd-', label='F1 Score')\n",
    "    plt.title('Model Performance for Each Epsilon')\n",
    "    plt.xlabel('Privacy Budget (ε)')\n",
    "    plt.ylabel('Performance Metric')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training curves for each epsilon\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for i, r in enumerate(results):\n",
    "        if 'training_history' in r and 'val_accuracy' in r['training_history']:\n",
    "            val_acc = r['training_history']['val_accuracy']\n",
    "            if isinstance(val_acc, list) and len(val_acc) > 0:\n",
    "                # Get x values (epochs)\n",
    "                epochs = list(range(1, len(val_acc) + 1))\n",
    "                label = f\"ε={r['epsilon']}\" if r['epsilon'] is not None else \"No DP\"\n",
    "                plt.plot(epochs, val_acc, marker='o', label=label)\n",
    "\n",
    "    plt.title('Validation Accuracy During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.xticks(np.arange(1, EPOCHS+1))\n",
    "    plt.ylim(0, 1.0)  # Accuracy is between 0 and 1\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('privacy_utility_tradeoff.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Create a summary table of performance metrics\n",
    "    performance_df = pd.DataFrame({\n",
    "        'Epsilon': ['No DP' if r['epsilon'] is None else f\"ε={r['epsilon']}\" for r in results],\n",
    "        'Accuracy': [r['accuracy'] for r in results],\n",
    "        'F1 Score': [r['f1_score'] for r in results],\n",
    "        'Best Val Accuracy': [r['val_accuracy'] for r in results]\n",
    "    })\n",
    "    \n",
    "    mia_rows = []\n",
    "    for r in results:\n",
    "        epsilon_label = 'No DP' if r['epsilon'] is None else f\"ε={r['epsilon']}\"\n",
    "        for epoch_result in r['mia_results']:\n",
    "            epoch = epoch_result[0]\n",
    "            metrics = epoch_result[1]\n",
    "            mia_rows.append({\n",
    "                'Epsilon': epsilon_label,\n",
    "                'Epoch': epoch,\n",
    "                'MIA Accuracy': metrics.get('accuracy', None),\n",
    "                'MIA AUC': metrics.get('auc', None),\n",
    "                'Threshold': metrics.get('threshold', None),\n",
    "                'Attack Type': metrics.get('attack_type', None),\n",
    "            })\n",
    "\n",
    "    mia_df = pd.DataFrame(mia_rows)\n",
    "    \n",
    "\n",
    "    return performance_df, mia_df\n",
    "\n",
    "# pull mia results\n",
    "def get_mia_metric(r, epoch, metric):\n",
    "    if 'mia_results' in r:\n",
    "        for ep, res in r['mia_results']:\n",
    "            if ep == epoch:\n",
    "                return res.get(metric, np.nan)\n",
    "    return np.nan\n",
    "\n",
    "# plot MIA results\n",
    "def plot_mia_results(results):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Subplot 1: Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for r in results:\n",
    "        if 'mia_results' not in r:\n",
    "            continue\n",
    "        eps_label = 'No DP' if r['epsilon'] is None else f\"ε={r['epsilon']}\"\n",
    "        epochs = [epoch for epoch, _ in r['mia_results']]\n",
    "        accs = [metrics['accuracy'] for _, metrics in r['mia_results']]\n",
    "        plt.plot(epochs, accs, label=eps_label, marker='o')\n",
    "    plt.title(\"MIA Accuracy Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(title='Epsilon', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Subplot 2: AUC\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for r in results:\n",
    "        if 'mia_results' not in r:\n",
    "            continue\n",
    "        eps_label = 'No DP' if r['epsilon'] is None else f\"ε={r['epsilon']}\"\n",
    "        epochs = [epoch for epoch, _ in r['mia_results']]\n",
    "        aucs = [metrics['auc'] for _, metrics in r['mia_results']]\n",
    "        plt.plot(epochs, aucs, label=eps_label, marker='o')\n",
    "    plt.title(\"MIA AUC Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(title='Epsilon', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "fe9bef310fc53e85",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Below cell provides epsilon values to test and calls run_dp_experiments to build models",
   "id": "646975bd4300d062"
  },
  {
   "metadata": {
    "id": "f21092715eae9c03",
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:10.581839Z",
     "start_time": "2025-04-09T01:11:16.264924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Set privacy budgets to test\n",
    "epsilon_values = [None, 10.0, 1.0, 0.1]\n",
    "\n",
    "# Run the experiments\n",
    "results = run_dp_experiments(train_encodings, train_labels, val_encodings, val_labels, test_encodings, test_labels, epsilons=epsilon_values, set_microbatch=1, clip_norm=best_clip_norm)"
   ],
   "id": "f21092715eae9c03",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualize and save results",
   "id": "cb65faa1348fda83"
  },
  {
   "metadata": {
    "id": "55296a0d987bb025",
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:11.137225Z",
     "start_time": "2025-04-09T04:06:10.587280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot and analyze results\n",
    "results_df, mia_results_df = plot_privacy_utility_tradeoff(results)\n",
    "print(\"Performance Summary:\")\n",
    "display(results_df)\n",
    "\n",
    "# MIA analysis\n",
    "display(mia_results_df)\n",
    "plot_mia_results(results)"
   ],
   "id": "55296a0d987bb025",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "id": "afc084b060c1b8fc",
    "ExecuteTime": {
     "end_time": "2025-04-09T04:06:11.145705Z",
     "start_time": "2025-04-09T04:06:11.138002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save results\n",
    "results_df.to_csv('performance_results.csv', index=False)\n",
    "mia_results_df.to_csv('mia_results.csv', index=False)"
   ],
   "id": "afc084b060c1b8fc",
   "execution_count": 21,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
