{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install dependencies\n",
    "# !pip install -q transformers==4.17\n",
    "# !pip install tensorflow-macos \n",
    "# !pip install tensorflow-metal\n",
    "# !pip install tensorflow==2.15.0 --quiet\n",
    "# !pip install tensorflow-text==2.15.0 --quiet\n",
    "# !pip install tensorflow-privacy\n",
    "# !pip install numpy==1.26.4\n",
    "# !pip install pandas==2.1.1\n"
   ],
   "id": "5-E2sfMdYE8L",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Suppress warnings from all Python libraries\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure basic logging level\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Target specific libraries\n",
    "for module in ['tensorflow', 'absl', 'transformers', 'tensorboard']:\n",
    "    logging.getLogger(module).setLevel(logging.ERROR)\n",
    "\n",
    "from transformers import logging as transformers_logging\n",
    "transformers_logging.set_verbosity_error()\n",
    "\n",
    "# Suppress the specific BERT layer warnings\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'"
   ],
   "id": "ca385959c79f25ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ],
   "id": "51c8ac0b9958ef55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "initial_id",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "outputId": "4b19d334-db27-4e16-a8ff-8b5c29479566"
   },
   "cell_type": "code",
   "source": [
    "# import libraries/modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow_privacy.privacy.optimizers import dp_optimizer_keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import TFBertModel, BertTokenizer, TFDistilBertModel, DistilBertTokenizer\n",
    "from tensorflow_privacy.privacy.analysis import compute_noise_from_budget_lib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from mia_callback import MembershipInferenceCallback\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow_privacy\n",
    "print(tensorflow_privacy.__version__)"
   ],
   "id": "fa4538f504ac4844",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "6c034bf52ca668ee"
   },
   "cell_type": "code",
   "source": [
    "MAX_LENGTH = 512\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 3"
   ],
   "id": "6c034bf52ca668ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "PWWTNngTYOMu"
   },
   "id": "PWWTNngTYOMu",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8482f4150c580251"
   },
   "cell_type": "code",
   "source": [
    "# load dataset\n",
    "reviews_df = pd.read_pickle('data/resto_reviews_3classes.pkl')"
   ],
   "id": "8482f4150c580251",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "ceae08293524ca92"
   },
   "cell_type": "code",
   "source": [
    "# shuffle dataframe\n",
    "reviews_df = reviews_df.sample(frac=1, random_state=333)\n",
    "# Reset index\n",
    "reviews_df = reviews_df.reset_index(drop=True)\n",
    "\n",
    "# Preview dataframe\n",
    "display(reviews_df)"
   ],
   "id": "ceae08293524ca92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "be0645df82cc8"
   },
   "cell_type": "code",
   "source": [
    "# Label encode forms\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the column\n",
    "reviews_df['sentiment_encoded'] = le.fit_transform(reviews_df['sentiment'])\n",
    "\n",
    "# Calculate the desired training set size (nearest multiple of 32 to 60% of data)\n",
    "total_samples = len(reviews_df)\n",
    "train_size = int(total_samples * 0.6)\n",
    "train_size_adjusted = (train_size // 32) * 32  # Round down to nearest multiple of 32\n",
    "\n",
    "# Use the adjusted size in your train_test_split\n",
    "train_ratio = train_size_adjusted / total_samples\n",
    "remaining_ratio = 1 - train_ratio\n",
    "\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(reviews_df['text'], reviews_df['sentiment_encoded'], test_size=remaining_ratio, random_state=123, stratify=reviews_df['sentiment_encoded'])\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.50, random_state=1234,\n",
    "    stratify=y_temp)\n",
    "\n",
    "# lowercase text if using uncased\n",
    "x_train = x_train.str.lower()\n",
    "x_val = x_val.str.lower()\n",
    "x_test = x_test.str.lower()\n",
    "\n",
    "n_examples = len(x_train)\n",
    "DELTA = round(1/n_examples, 5)\n"
   ],
   "id": "be0645df82cc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "76a6ea9c8d50ce8e"
   },
   "cell_type": "code",
   "source": [
    "# Create bert encoder function\n",
    "def bert_encode(texts, tokenizer, max_len=MAX_LENGTH):\n",
    "    # Convert texts to strings\n",
    "    texts_str = [str(text) for text in texts]\n",
    "\n",
    "    # Tokenize texts\n",
    "    tokenized = tokenizer(texts_str,\n",
    "                         max_length=max_len,\n",
    "                         truncation=True,\n",
    "                         padding='max_length',\n",
    "                         return_tensors='tf')\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# select tokenizer\n",
    "def select_tokenizer(pretrained):\n",
    "    tokenizers = {\n",
    "        'bert-base-cased': BertTokenizer.from_pretrained(pretrained),\n",
    "        'bert-base-uncased': BertTokenizer.from_pretrained(pretrained),\n",
    "        'distilbert-base-uncased': DistilBertTokenizer.from_pretrained(pretrained),\n",
    "        'nlptown/bert-base-multilingual-uncased-sentiment': BertTokenizer.from_pretrained(pretrained),\n",
    "    }\n",
    "    tokenizer = tokenizers[pretrained]\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names=['Negative', 'Neutral', 'Positive']):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Create heatmap with numerical values inside cells\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    # Add accuracy in each cell as percentage\n",
    "    total = np.sum(cm)\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm[i])):\n",
    "            plt.text(j + 0.5, i + 0.7, f'({cm[i, j]/total*100:.1f}%)',\n",
    "                    ha='center', color='black' if cm[i, j] < cm.max()/2 else 'white')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return plt"
   ],
   "id": "76a6ea9c8d50ce8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "d2b0e213f32a1e04"
   },
   "cell_type": "code",
   "source": [
    "# Select pretrained\n",
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "# Load tokenizer\n",
    "bert_tokenizer = select_tokenizer(model_checkpoint)\n",
    "\n",
    "# Tokenize dataset\n",
    "train_encodings = bert_encode(x_train, bert_tokenizer)\n",
    "val_encodings = bert_encode(x_val, bert_tokenizer)\n",
    "test_encodings = bert_encode(x_test, bert_tokenizer)\n",
    "train_labels = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
    "val_labels = tf.convert_to_tensor(y_val, dtype=tf.int32)\n",
    "test_labels = tf.convert_to_tensor(y_test, dtype=tf.int32)\n"
   ],
   "id": "d2b0e213f32a1e04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "47e612f8678456fa"
   },
   "cell_type": "code",
   "source": [
    "print(\"Type of train_encodings:\", type(train_encodings))\n",
    "print(\"Keys of train_encodings:\", train_encodings.keys())\n",
    "print(\"train_y shape:\", train_labels.shape)\n",
    "print(\"train_y dtype:\", train_labels.dtype)\n",
    "print(\"train_y unique values:\", np.unique(train_labels))\n",
    "print(\"Number of classes:\", len(np.unique(train_labels)))"
   ],
   "id": "47e612f8678456fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "7f738bcdec6657cd"
   },
   "cell_type": "markdown",
   "source": [
    "## Model with Differential Privacy Integration"
   ],
   "id": "7f738bcdec6657cd"
  },
  {
   "metadata": {
    "id": "348bc13154fe2511"
   },
   "cell_type": "code",
   "source": [
    "def build_dp_model(bert_base,\n",
    "                trainable=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                num_classes=NUM_CLASSES,\n",
    "                hidden_sizes=[128, 64],\n",
    "                dropout=[0.3],\n",
    "                learning_rate=0.0001,\n",
    "                epsilon=None,\n",
    "                delta=DELTA,\n",
    "                l2_norm_clip=1.0,\n",
    "                num_microbatches=32):\n",
    "\n",
    "    # Select appropriate model type based on the checkpoint name\n",
    "    if 'distilbert' in bert_base:\n",
    "        bert_model = TFDistilBertModel.from_pretrained(bert_base)\n",
    "    else:\n",
    "        bert_model = TFBertModel.from_pretrained(bert_base)\n",
    "\n",
    "    # Unfreeze all layers to make model overfit\n",
    "    # Overfitting provides more evidence that DP is working\n",
    "    bert_model.trainable = trainable\n",
    "\n",
    "    # Unfreeze only last layers to make model not overfit\n",
    "    # if 'distilbert' in bert_base:\n",
    "    #     # Unfreeze last 4 transformer layers (layers 2, 3, 4, 5)\n",
    "    #     for i in range(-4, 0):  \n",
    "    #         bert_model.distilbert.transformer.layer[i].trainable = True\n",
    "    # else:\n",
    "    #     # For BERT (12 layers), unfreeze last 4\n",
    "    #     for i in range(-4, 0):\n",
    "    #         bert_model.encoder.layer[i].trainable = True\n",
    "\n",
    "    # Create input layers\n",
    "    input_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='input_ids_layer')\n",
    "    attention_mask = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask_layer')\n",
    "\n",
    "    if 'distilbert' in bert_base:\n",
    "        bert_inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "        model_inputs = [input_ids, attention_mask]\n",
    "    else:\n",
    "        token_type_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "        bert_inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "        model_inputs = [input_ids, token_type_ids, attention_mask]\n",
    "\n",
    "    bert_output = bert_model(bert_inputs)\n",
    "\n",
    "    # Get pooler output\n",
    "    if 'distilbert' in bert_base:\n",
    "        sequence_output = bert_output[0]\n",
    "        pooler_output = sequence_output[:, 0, :]\n",
    "    else:\n",
    "        pooler_output = bert_output[1]\n",
    "\n",
    "    # Add multiple hidden layers with decreasing sizes\n",
    "    hidden = pooler_output\n",
    "    for i, hidden_size in enumerate(hidden_sizes):\n",
    "        hidden = tf.keras.layers.Dense(hidden_size, activation='relu')(hidden)\n",
    "        # if i < 1:\n",
    "        #     hidden = tf.keras.layers.Dropout(dropout[i], name=f'dropout_{i}')(hidden)\n",
    "\n",
    "    class_output = tf.keras.layers.Dense(num_classes, activation=None, name=\"classification_layer\")(hidden)\n",
    "\n",
    "    model = tf.keras.Model(inputs=model_inputs, outputs=class_output)\n",
    "    \n",
    "    \n",
    "    if epsilon is not None:\n",
    "        # Calculate noise multiplier\n",
    "        noise = compute_noise_from_budget_lib.compute_noise(n_examples,\n",
    "                                                            BATCH_SIZE,\n",
    "                                                            epsilon,\n",
    "                                                            EPOCHS,\n",
    "                                                            delta,\n",
    "                                                            0.1)\n",
    "\n",
    "        optimizer = dp_optimizer_keras.DPKerasAdamOptimizer(\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise,\n",
    "            num_microbatches=num_microbatches,\n",
    "            learning_rate=learning_rate\n",
    "        )\n",
    "\n",
    "        # For DP: Use non-reduced sparse categorical loss\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Standard optimizer (no privacy)\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        # For non-DP: Use standard reduced loss\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        print(\"No privacy protection applied\")\n",
    "\n",
    "    # Compile the model once with the chosen optimizer\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Obtain instance of pretrained model\n",
    "dp_model = build_dp_model(model_checkpoint)\n",
    "dp_model.summary()"
   ],
   "id": "348bc13154fe2511",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "37e7dc1b5df2a1d5"
   },
   "cell_type": "markdown",
   "source": [
    "## Experiments\n"
   ],
   "id": "37e7dc1b5df2a1d5"
  },
  {
   "metadata": {
    "id": "7d181809452176d7"
   },
   "cell_type": "code",
   "source": [
    "# Early stopping is currently not implemented during DP experiments so we can see how models act through all epochs\n",
    "# Create early stopping callback \n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n"
   ],
   "id": "7d181809452176d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "dcab80363126c1c9"
   },
   "cell_type": "code",
   "source": [
    "def run_dp_hyperparameter_study(train_x, train_y, val_x, val_y, test_x, test_y, model_checkpoint, batch_size=32, early_stop=None, ablation_epochs=1):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Prepare inputs based on model type\n",
    "    if 'distilbert' in model_checkpoint:\n",
    "        train_inputs = [train_x.input_ids, train_x.attention_mask]\n",
    "        val_inputs = [val_x.input_ids, val_x.attention_mask]\n",
    "        test_inputs = [test_x.input_ids, test_x.attention_mask]\n",
    "    else:\n",
    "        train_inputs = [train_x.input_ids, train_x.token_type_ids, train_x.attention_mask]\n",
    "        val_inputs = [val_x.input_ids, val_x.token_type_ids, val_x.attention_mask]\n",
    "        test_inputs = [test_x.input_ids, test_x.token_type_ids, test_x.attention_mask]\n",
    "\n",
    "\n",
    "    # Clipping norm study\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Pick clipping norm\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    clip_norms = [0.3, 0.5, 1.0, 1.5]\n",
    "    for clip_norm in clip_norms:\n",
    "        print(f\"\\nTesting clipping norm: {clip_norm}\")\n",
    "\n",
    "        # Use a fixed epsilon and microbatches for this test\n",
    "        epsilon = 1.0\n",
    "        microbatches = 32\n",
    "\n",
    "        # Build model with the current clipping norm\n",
    "        model = build_dp_model(\n",
    "            model_checkpoint,\n",
    "            epsilon=epsilon,\n",
    "            l2_norm_clip=clip_norm,\n",
    "            num_microbatches=microbatches\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Fit model\n",
    "        history = model.fit(\n",
    "            train_inputs,\n",
    "            train_y,\n",
    "            validation_data=(val_inputs, val_y),\n",
    "            batch_size=batch_size,\n",
    "            epochs=ablation_epochs,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_loss, test_acc = model.evaluate(test_inputs, test_y, verbose=0)\n",
    "\n",
    "        # Get predictions and calculate F1 score\n",
    "        y_pred = np.argmax(model.predict(test_inputs), axis=1)\n",
    "        f1 = f1_score(test_y, y_pred, average='weighted')\n",
    "\n",
    "        print(f\"Results for clip_norm={clip_norm}:\")\n",
    "        print(f\"Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            'parameter': 'clip_norm',\n",
    "            'value': clip_norm,\n",
    "            'epsilon': epsilon,\n",
    "            'microbatches': microbatches,\n",
    "            'accuracy': test_acc,\n",
    "            'f1_score': f1,\n",
    "            'val_accuracy': max(history.history['val_accuracy']),\n",
    "        })\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    # Find best clip_norm\n",
    "    best_clip_results = [r for r in results if r['parameter'] == 'clip_norm']\n",
    "    best_clip_norm = max(best_clip_results, key=lambda x: x['f1_score'])['value']\n",
    "    print(f\"\\nBest configuration:\")\n",
    "    print(f\"- Clipping norm: {best_clip_norm}\")\n",
    "\n",
    "    # Create DataFrame from results\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Plot results\n",
    "    plot_ablation_results(results_df)\n",
    "\n",
    "    return results_df, best_clip_norm\n",
    "\n",
    "def plot_ablation_results(results_df):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "\n",
    "    # 1. Plot clipping norm results\n",
    "    clip_norm_results = results_df[results_df['parameter'] == 'clip_norm']\n",
    "    plt.plot(clip_norm_results['value'], clip_norm_results['accuracy'], 'o-', label='Accuracy')\n",
    "    plt.plot(clip_norm_results['value'], clip_norm_results['f1_score'], 'd-', label='F1 Score')\n",
    "    plt.title('Effect of Clipping Norm (C) on Model Performance')\n",
    "    plt.xlabel('Clipping Norm')\n",
    "    plt.ylabel('Performance Metric')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dp_hyperparameter_ablation.png', dpi=300)\n",
    "    plt.show()"
   ],
   "id": "dcab80363126c1c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "198b1da68379f41b"
   },
   "cell_type": "code",
   "source": [
    "# Run the simplified hyperparameter ablation study\n",
    "clipnorm_results_df, best_clip_norm = run_dp_hyperparameter_study(\n",
    "    train_encodings, train_labels,\n",
    "    val_encodings, val_labels,\n",
    "    test_encodings, test_labels,\n",
    "    model_checkpoint,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    early_stop=early_stopping,\n",
    "    ablation_epochs=EPOCHS//2\n",
    ")\n",
    "\n",
    "# Display summary table\n",
    "display(clipnorm_results_df)\n",
    "\n",
    "print(f\"- Best clipping norm: {best_clip_norm}\")"
   ],
   "id": "198b1da68379f41b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "9db1a4b167de8746"
   },
   "cell_type": "markdown",
   "source": [],
   "id": "9db1a4b167de8746"
  },
  {
   "metadata": {
    "id": "fe9bef310fc53e85"
   },
   "cell_type": "code",
   "source": [
    "# Run experiments with different privacy budgets\n",
    "def run_dp_experiments(train_x, train_y, val_x, val_y, test_x, test_y, epsilons=[None], set_microbatch=32, clip_norm=0.5):\n",
    "    results = []\n",
    "    # Set up inputs based on model type\n",
    "    if 'distilbert' in model_checkpoint:\n",
    "        train_inputs = [train_x.input_ids, train_x.attention_mask]\n",
    "        val_inputs = [val_x.input_ids, val_x.attention_mask]\n",
    "        test_inputs = [test_x.input_ids, test_x.attention_mask]\n",
    "    else:\n",
    "        train_inputs = [train_x.input_ids, train_x.token_type_ids, train_x.attention_mask]\n",
    "        val_inputs = [val_x.input_ids, val_x.token_type_ids, val_x.attention_mask]\n",
    "        test_inputs = [test_x.input_ids, test_x.token_type_ids, test_x.attention_mask]\n",
    "\n",
    "\n",
    "    for epsilon in epsilons:\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"Training with epsilon = {epsilon if epsilon is not None else 'No DP'}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "\n",
    "        # Build model with specified privacy budget\n",
    "        model = build_dp_model(\n",
    "            model_checkpoint,\n",
    "            epsilon=epsilon,\n",
    "            l2_norm_clip=clip_norm,\n",
    "            num_microbatches=set_microbatch\n",
    "        )\n",
    "        \n",
    "        mia_callback = MembershipInferenceCallback(\n",
    "            train_inputs=train_inputs,\n",
    "            train_labels=train_y,\n",
    "            val_inputs=val_inputs,\n",
    "            val_labels=val_y,\n",
    "            run_epochs=[1, 3, 5])\n",
    "        \n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_inputs,\n",
    "            train_y,\n",
    "            validation_data=(val_inputs, val_y),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[mia_callback],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_loss, test_acc = model.evaluate(test_inputs, test_y, verbose=0)\n",
    "\n",
    "        # Get predictions and calculate F1 score\n",
    "        y_pred = np.argmax(model.predict(test_inputs), axis=1)\n",
    "        f1 = f1_score(test_y, y_pred, average='weighted')\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        cm = confusion_matrix(test_y, y_pred)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'epsilon': epsilon,\n",
    "            'accuracy': test_acc,\n",
    "            'f1_score': f1,\n",
    "            'val_accuracy': max(history.history['val_accuracy']),\n",
    "            'training_history': history.history,\n",
    "            'mia_results': mia_callback.attack_results,\n",
    "            'mia_predictions': mia_callback.epoch_predictions,\n",
    "            'epoch_f1_scores': mia_callback.f1_scores,  # ← NEW: Add per-epoch F1s\n",
    "        })\n",
    "\n",
    "        print(f\"Test results for epsilon={epsilon if epsilon is not None else 'No DP'}:\")\n",
    "        print(f\"Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "        # Show confusion matrix\n",
    "        plot_confusion_matrix(cm)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "# Plot results to visualize privacy-utility tradeoff\n",
    "def plot_privacy_utility_tradeoff(results):\n",
    "    # Convert None to \"No DP\" for plotting\n",
    "    epsilon_labels = [\"No DP\" if r['epsilon'] is None else str(r['epsilon']) for r in results]\n",
    "    accuracies = [r['accuracy'] for r in results]\n",
    "    f1_scores = [r['f1_score'] for r in results]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot privacy-utility tradeoff\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epsilon_labels, accuracies, 'o-', label='Accuracy')\n",
    "    plt.plot(epsilon_labels, f1_scores, 'd-', label='F1 Score')\n",
    "    plt.title('Privacy-Utility Tradeoff')\n",
    "    plt.xlabel('Privacy Budget (ε)')\n",
    "    plt.ylabel('Performance Metric')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training curves for each epsilon\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for i, r in enumerate(results):\n",
    "        if 'training_history' in r and 'val_accuracy' in r['training_history']:\n",
    "            val_acc = r['training_history']['val_accuracy']\n",
    "            # Ensure val_acc is a list and contains valid values to plot\n",
    "            if isinstance(val_acc, list) and len(val_acc) > 0:\n",
    "                # Get x values (epochs)\n",
    "                epochs = list(range(1, len(val_acc) + 1))\n",
    "                label = f\"ε={r['epsilon']}\" if r['epsilon'] is not None else \"No DP\"\n",
    "                plt.plot(epochs, val_acc, marker='o', label=label)\n",
    "\n",
    "    plt.title('Validation Accuracy During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.xticks(np.arange(1, EPOCHS+1))\n",
    "    plt.ylim(0, 1.0)  # Accuracy is between 0 and 1\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('privacy_utility_tradeoff.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Create a summary table of performance metrics\n",
    "    performance_df = pd.DataFrame({\n",
    "        'Epsilon': ['No DP' if r['epsilon'] is None else f\"ε={r['epsilon']}\" for r in results],\n",
    "        'Accuracy': [r['accuracy'] for r in results],\n",
    "        'F1 Score': [r['f1_score'] for r in results],\n",
    "        'Best Val Accuracy': [r['val_accuracy'] for r in results]\n",
    "    })\n",
    "    \n",
    "    mia_rows = []\n",
    "    for r in results:\n",
    "        epsilon_label = 'No DP' if r['epsilon'] is None else f\"ε={r['epsilon']}\"\n",
    "        for epoch_result in r['mia_results']:\n",
    "            epoch = epoch_result[0]\n",
    "            metrics = epoch_result[1]\n",
    "            mia_rows.append({\n",
    "                'Epsilon': epsilon_label,\n",
    "                'Epoch': epoch,\n",
    "                'MIA Accuracy': metrics.get('accuracy', None),\n",
    "                'MIA AUC': metrics.get('auc', None),\n",
    "                'Threshold': metrics.get('threshold', None),\n",
    "                'Attack Type': metrics.get('attack_type', None),\n",
    "            })\n",
    "\n",
    "    mia_df = pd.DataFrame(mia_rows)\n",
    "    \n",
    "\n",
    "    return performance_df, mia_df\n",
    "\n",
    "\n",
    "def get_mia_metric(r, epoch, metric):\n",
    "    if 'mia_results' in r:\n",
    "        for ep, res in r['mia_results']:\n",
    "            if ep == epoch:\n",
    "                return res.get(metric, np.nan)\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def plot_mia_results(results):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Subplot 1: Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for r in results:\n",
    "        if 'mia_results' not in r:\n",
    "            continue\n",
    "        eps_label = 'No DP' if r['epsilon'] is None else f\"ε={r['epsilon']}\"\n",
    "        epochs = [epoch for epoch, _ in r['mia_results']]\n",
    "        accs = [metrics['accuracy'] for _, metrics in r['mia_results']]\n",
    "        plt.plot(epochs, accs, label=eps_label, marker='o')\n",
    "    plt.title(\"MIA Accuracy Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(title='Epsilon', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Subplot 2: AUC\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for r in results:\n",
    "        if 'mia_results' not in r:\n",
    "            continue\n",
    "        eps_label = 'No DP' if r['epsilon'] is None else f\"ε={r['epsilon']}\"\n",
    "        epochs = [epoch for epoch, _ in r['mia_results']]\n",
    "        aucs = [metrics['auc'] for _, metrics in r['mia_results']]\n",
    "        plt.plot(epochs, aucs, label=eps_label, marker='o')\n",
    "    plt.title(\"MIA AUC Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(title='Epsilon', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "fe9bef310fc53e85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "f21092715eae9c03"
   },
   "cell_type": "code",
   "source": [
    "#Set privacy budgets to test\n",
    "epsilon_values = [0.1]\n",
    "\n",
    "\n",
    "# Run the experiments\n",
    "results = run_dp_experiments(train_encodings, train_labels, val_encodings, val_labels, test_encodings, test_labels, epsilons=epsilon_values, set_microbatch=1, clip_norm=best_clip_norm)\n"
   ],
   "id": "f21092715eae9c03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "55296a0d987bb025"
   },
   "cell_type": "code",
   "source": [
    "# Plot and analyze results\n",
    "results_df, mia_results_df = plot_privacy_utility_tradeoff(results)\n",
    "print(\"Performance Summary:\")\n",
    "display(results_df)\n",
    "\n",
    "# MIA analysis\n",
    "display(mia_results_df)\n",
    "plot_mia_results(results)"
   ],
   "id": "55296a0d987bb025",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "afc084b060c1b8fc"
   },
   "cell_type": "code",
   "source": [
    "# Save results\n",
    "results_df.to_csv('performance_results.csv', index=False)\n",
    "mia_results_df.to_csv('mia_results.csv', index=False)"
   ],
   "id": "afc084b060c1b8fc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
